from util import *import numpy as npfrom sklearn.svm import SVCfrom sklearn import preprocessingimport warnings #Performs cross validation to choose the best hyperparametersdef cross_validation():    warnings.filterwarnings("ignore")    data, target, identity = load_labeled_data()    data_scaled = scale(data)    data_norm = preprocessing.normalize(data_scaled, norm='l2')    data_equ = equalize(data_norm)        for ker in ['linear', 'poly', 'rbf']:        scores = []        for C in range(1,11):            score = []            for gamma in [0.001, 0.005, 0.01, 0.05, 0.1, 0.5]:                    x_train, x_test, y_train, y_test =  train_test_split(data_equ,                                                                         target, identity)                    clf = SVC(C=C, kernel=ker, gamma=gamma)                    clf.fit(x_train, y_train)                    score.append(clf.score(x_test, y_test))            print score            scores.append(score)            scores = np.array(scores)        print "max: ", np.max(scores), " mean: ", np.mean(scores), " var: ", np.var(scores)# Loop cross validation 10 times on the given hyperparametersdef loop_parameter(C, gamma):    warnings.filterwarnings("ignore")    data, target, identity = load_labeled_data()    data_scaled = scale(data)    data_norm = preprocessing.normalize(data_scaled, norm='l2')    data_equ = equalize(data_norm)        score = []    for i in range(10):        x_train, x_test, y_train, y_test =  train_test_split(data_equ,                                                             target, identity)        clf = SVC(C=C, gamma=gamma)        clf.fit(x_train, y_train)        score.append(clf.score(x_test, y_test))        score = np.array(score)    print score    print "max ", np.max(score), " mean: ", np.mean(score), " var: ", np.var(score)    #run SVC model to create predictions given the hyperparametersdef run_svc(C, gamma):    warnings.filterwarnings("ignore")#load labeled image       data, target, identity = load_labeled_data()    #Standardize the data       data_scaled = scale(data)    #normalize the data    data_norm = preprocessing.normalize(data_scaled, norm='l2')    #equalize the data    data_equ = equalize(data_norm)# load public test image        public_test = load_public_test()    #Standardize the data     public_scaled = scale(public_test)    #normalize the data        public_norm = preprocessing.normalize(public_scaled, norm='l2')    #equalize the data    public_equ = equalize(public_norm)# load hidden test image     hidden_test = load_hidden_test()    #Standardize the data     hidden_scaled = scale(hidden_test)    #normalize the data     hidden_norm = preprocessing.normalize(hidden_scaled, norm='l2')    #equalize the data    hidden_equ = equalize(hidden_norm)# concatenate public test image and hidden test image        test = np.concatenate((public_equ, hidden_equ), axis=0)# Train a SVM classification model    clf = SVC(C=C, gamma=gamma)    clf.fit(data_equ, target)    pred = clf.predict(test)#write result to csv file    write_results(pred, 'svm_res.csv')    if __name__ == '__main__':#    cross_validation()#    loop_parameter(4, 0.01)    run_svc(4, 0.01)        